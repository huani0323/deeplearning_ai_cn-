{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立循环神经网络-逐步\n",
    "\n",
    "欢迎来到课程5的第一个作业！在此作业中，您将以numpy实现您的第一个循环神经网络。\n",
    "\n",
    "递归神经网络（RNN）对自然语言处理和其他序列任务非常有效，因为它们具有“内存”。他们可以一次读取一个输入$x^{\\langle t \\rangle}$（例如单词），并通过从一个时间步长传递到下一个时间步长的隐藏层激活来记住一些信息/上下文。这允许单向RNN提取过去的信息以处理以后的输入。双向RNN可以借鉴过去和未来的环境。\n",
    "\n",
    "**符号**：\n",
    "- 上标$[l]$表示与$l^{th}$层关联的对象。\n",
    "    - 示例：$a^{[4]}$是$4^{th}$层激活。 $W^{[5]}$和$b^{[5]}$是$5^{th}$层参数。\n",
    "\n",
    "- 上标$(i)$表示与 $i^{th}$示例关联的对象。\n",
    "    - 示例：$x^{(i)}$ 是$i^{th}$训练样本输入。\n",
    "\n",
    "- 上标 $\\langle t \\rangle$ 表示在$t^{th}$时间步的对象。\n",
    "    - 示例：$x^{\\langle t \\rangle}$是在$t^{th}$时间步长处的输入x。 $x^{(i)\\langle t \\rangle}$是样本$i$的$t^{th}$时间步长的输入。\n",
    "    \n",
    "- 下标$i$表示向量的$i^{th}$ 条目。\n",
    "    - 示例：$a^{[l]}_i$表示层l中激活的ith条目。\n",
    "\n",
    "我们假设您已经熟悉`numpy`和/或已经完成了以前的专业课程。让我们开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，让我们导入在此分配过程中需要的所有软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnn_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-基本递归神经网络的正向传播\n",
    "\n",
    "本周晚些时候，您将使用RNN生成音乐。 您将实现的基本RNN具有以下结构。 在此示例中，$T_x = T_y$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RNN.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 1**: Basic RNN model </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是实现RNN的方法：\n",
    "\n",
    "**步骤**：\n",
    "1.实施RNN的一个时间步所需的计算。\n",
    "2.在 $T_x$时间步长上实现循环，以便一次处理所有输入。\n",
    "\n",
    "我们开始吧！\n",
    "\n",
    "## 1.1-RNN单元\n",
    "\n",
    "循环神经网络可以看作是单个细胞的重复。您首先要在单个时间步上实现计算。下图描述了RNN单元的单个时间步的操作。\n",
    "\n",
    "<img src=\"images/rnn_step_forward.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **图2 **：基本RNN单元。将$x^{\\langle t \\rangle}$ (当前输入) 和 $a^{\\langle t - 1\\rangle}$ （包含过去信息的先前隐藏状态）作为输入，并输出$a^{\\langle t \\rangle}$赋予下一个RNN单元，还用于预测$y^{\\langle t \\rangle}$ </center></caption>\n",
    "\n",
    "**练习**：实现图（2）中描述的RNN单元。\n",
    "\n",
    "**说明**：\n",
    "1. 使用tanh激活计算隐藏状态： $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$。\n",
    "2. 使用新的隐藏状态$a^{\\langle t \\rangle}$，计算预测 $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$。我们为您提供了一个函数：`softmax`。\n",
    "3. 将$(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$存储在cache中\n",
    "4. 返回$a^{\\langle t \\rangle}$ , $y^{\\langle t \\rangle}$ 和cache\n",
    "\n",
    "我们将对$m$个示例进行矢量化处理。因此，$x^{\\langle t \\rangle}$将具有维度$(n_x,m)$，而$a^{\\langle t \\rangle}$ 将具有维度$(n_a,m)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: rnn_cell_forward\n",
    "\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell as described in Figure (2)\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    ### START CODE HERE ### (≈2 lines)\n",
    "    # compute next activation state using the formula given above\n",
    "    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
    "    # compute output of the current cell using the formula given above\n",
    "    yt_pred = softmax(np.dot(Wya, a_prev) + by)   \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # store values you need for backward propagation in cache\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
      " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
      "a_next.shape =  (5, 10)\n",
      "yt_pred[1] = [0.11805736 0.00150244 0.16286607 0.99959053 0.4818786  0.00199492\n",
      " 0.00664413 0.95503173 0.04582838 0.99990195]\n",
      "yt_pred.shape =  (2, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"yt_pred[1] =\", yt_pred[1])\n",
    "print(\"yt_pred.shape = \", yt_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**: \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **a_next[4]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
    " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **a_next.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **yt[1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212\n",
    "  0.36920224  0.9966312   0.9982559   0.17746526]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **yt.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (2, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - RNN前向\n",
    "\n",
    "您可以将RNN视为刚刚构建的单元的重复。如果您输入的数据序列经过10个时间步长，则将复制RNN单元10次。每个单元格都将前一个单元格（$a^{\\langle t-1 \\rangle}$）的隐藏状态和当前时间步的输入数据（$x^{\\langle t \\rangle}$）作为输入。它为此时间步长输出隐藏状态（$a^{\\langle t \\rangle}$）和预测（$y^{\\langle t \\rangle}$）。\n",
    "\n",
    "\n",
    "<img src=\"images/rnn.png\" style=\"width:800px;height:300px;\">\n",
    "<caption><center> **图3 **：基本RNN。输入序列$x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$在$T_x$时间步长上结转。网络输出$y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>\n",
    "\n",
    "\n",
    "\n",
    "**练习**：对图3所示的RNN的前向传播进行编码。\n",
    "\n",
    "**说明**：\n",
    "1. 创建一个零向量（$a$），它将存储RNN计算出的所有隐藏状态。\n",
    "2. 将“下一个”隐藏状态初始化为$a_0$（初始隐藏状态）。\n",
    "3. 开始遍历每个时间步，您的增量索引为$t$：\n",
    "    - 通过运行`rnn_step_forward`更新“下一个”隐藏状态和缓存。\n",
    "    - 将“下一个”隐藏状态存储在 $a$中（$t^{th}$位置）\n",
    "    - 将预测存储在y中\n",
    "    - 将缓存添加到缓存列表中\n",
    "4. 返回$a$, $y$和缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: rnn_forward\n",
    "\n",
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize \"caches\" which will contain the list of all caches\n",
    "    caches = []\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and Wy\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize \"a\" and \"y\" with zeros (≈2 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next (≈1 line)\n",
    "    a_next = a0\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, compute the prediction, get the cache (≈1 line)\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next,parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y (≈1 line)\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        # Append \"cache\" to \"caches\" (≈1 line)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][1] =  [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
      "a.shape =  (5, 10, 4)\n",
      "y_pred[1][3] = [0.96251083 0.79560373 0.86224861 0.11118257]\n",
      "y_pred.shape =  (2, 10, 4)\n",
      "caches[1][1][3] = [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
    "print(\"a[4][1] = \", a[4][1])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
    "print(\"y_pred.shape = \", y_pred.shape)\n",
    "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **a[4][1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **a.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10, 4)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **y[1][3]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.79560373  0.86224861  0.11118257  0.81515947]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **y.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (2, 10, 4)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cache[1][1][3]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **len(cache)**:\n",
    "        </td>\n",
    "        <td>\n",
    "           2\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜你！ 您已经从头开始成功构建了递归神经网络的正向传播。 这对于某些应用程序将足够好，但是会遇到梯度消失的问题。 因此，当每个输出$y^{\\langle t \\rangle}$可以主要使用“本地”上下文进行估算（即来自输入$x^{\\langle t' \\rangle}$的信息，其中$t'$是 距离$t$不太远）。\n",
    "\n",
    "在下一部分中，您将构建一个更复杂的LSTM模型，该模型更适合解决逐渐消失的梯度。 LSTM将能够更好地记住一条信息并将其保存许多时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 长短期记忆（LSTM）网络\n",
    "\n",
    "下图显示了LSTM单元的操作。\n",
    "\n",
    "<img src=\"images/LSTM.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> **图4 **：LSTM单元。这会在每个时间步长跟踪并更新“单元状态”或存储变量$c^{\\langle t \\rangle}$，这可能与$a^{\\langle t \\rangle}$不同。</center></caption>\n",
    "\n",
    "与上面的RNN示例类似，您将以单个时间步实现LSTM单元开始。然后，您可以从for循环内部迭代调用它，使其具有$T_x$时间步长的输入。\n",
    "\n",
    "### 关于门\n",
    "\n",
    "#### -遗忘门\n",
    "\n",
    "为了便于说明，假设我们正在阅读一段文本中的单词，并希望使用LSTM来跟踪语法结构，例如主题是单数还是复数。如果主题从单数变为复数，我们需要找到一种方法来摆脱以前存储的单/复数状态的存储值。在LSTM中，遗忘门使我们可以这样做：\n",
    "\n",
    "$$\\Gamma_f^{\\langle t \\rangle} = \\sigma(W_f[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_f)\\tag{1} $$\n",
    "\n",
    "\n",
    "在这里，$W_f$是控制遗忘门行为的权重。我们将$[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}]$连接起来并乘以$W_f$。上面的等式导致向量 $\\Gamma_f^{\\langle t \\rangle}$的值介于0到1之间。该遗忘门向量将逐元素乘以先前的单元状态$c^{\\langle t-1 \\rangle}$。因此，如果$\\Gamma_f^{\\langle t \\rangle}$的值之一为0（或接近于0），则意味着LSTM应在的相应组件中删除该信息（例如，单数主题）。 $c^{\\langle t-1 \\rangle}$。如果值之一为1，则它将保留信息。\n",
    "\n",
    "#### -更新门\n",
    "\n",
    "一旦我们忘记了所讨论的主题是单数，就需要找到一种更新它的方法，以反映新主题现在是复数。这是更新门的公式：\n",
    "\n",
    "$$\\Gamma_u^{\\langle t \\rangle} = \\sigma(W_u[a^{\\langle t-1 \\rangle}, x^{\\{t\\}}] + b_u)\\tag{2} $$ \n",
    "\n",
    "\n",
    "类似于忘记门，这里 $\\Gamma_u^{\\langle t \\rangle}$再次是0到1之间的值的向量。这将与$\\tilde{c}^{\\langle t \\rangle}$，以便计算$c^{\\langle t \\rangle}$。\n",
    "\n",
    "#### -更新单元格\n",
    "\n",
    "要更新新主题，我们需要创建一个新的数字向量，可以将其添加到先前的单元格状态中。我们使用的等式是：\n",
    "\n",
    "$$ \\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c)\\tag{3} $$\n",
    "\n",
    "\n",
    "最后，新的单元状态为：\n",
    "\n",
    "$$ c^{\\langle t \\rangle} = \\Gamma_f^{\\langle t \\rangle}* c^{\\langle t-1 \\rangle} + \\Gamma_u^{\\langle t \\rangle} *\\tilde{c}^{\\langle t \\rangle} \\tag{4} $$\n",
    "\n",
    "\n",
    "#### -输出门\n",
    "\n",
    "为了确定我们将使用哪些输出，我们将使用以下两个公式：\n",
    "\n",
    "$$ \\Gamma_o^{\\langle t \\rangle}=  \\sigma(W_o[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_o)\\tag{5}$$ \n",
    "$$ a^{\\langle t \\rangle} = \\Gamma_o^{\\langle t \\rangle}* \\tanh(c^{\\langle t \\rangle})\\tag{6} $$\n",
    "\n",
    "\n",
    "在等式5中，您决定使用S型函数输出什么；在等式6中，您将其乘以先前状态的 tanh tanh。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - LSTM单元\n",
    "\n",
    "**锻炼**：实施图（3）中描述的LSTM单元。\n",
    "\n",
    "**说明**：\n",
    "1. 将 $a^{\\langle t-1 \\rangle}$和$x^{\\langle t \\rangle}$连接在一个矩阵中：$concat = \\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle} \\end{bmatrix}$\n",
    "2. 计算所有公式2-6。 您可以使用`sigmoid（）`（提供）和`np.tanh（）`。\n",
    "3. 计算预测$y^{\\langle t \\rangle}$。 您可以使用`softmax（）`（提供）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: lstm_cell_forward\n",
    "\n",
    "def lstm_cell_forward(xt, a6prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implement a single forward step of the LSTM-cell as described in Figure (4)\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        Wi -- Weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bi -- Bias of the save gate, numpy array of shape (n_a, 1)\n",
    "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        bc --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        Wo -- Weight matrix of the focus gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bo --  Bias of the focus gate, numpy array of shape (n_a, 1)\n",
    "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "                        \n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    c_next -- next memory state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "    \n",
    "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilda),\n",
    "          c stands for the memory value\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # Retrieve dimensions from shapes of xt and Wy\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Concatenate a_prev and xt (≈3 lines)\n",
    "    concat = np.zeros((n_x+n_a, m))\n",
    "    concat[: n_a, :] = a_prev\n",
    "    concat[n_a :, :] = xt\n",
    "\n",
    "    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)\n",
    "    ft = sigmoid(np.dot(Wf, concat) + bf)\n",
    "    it = sigmoid(np.dot(Wi, concat) + bi)\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc)\n",
    "    c_next = np.multiply(ft,c_prev) + np.multiply(it,cct)\n",
    "    ot = sigmoid(np.dot(Wo, concat) + bo)\n",
    "    a_next = ot * np.tanh(c_next)\n",
    "    \n",
    "    # Compute prediction of the LSTM cell (≈1 line)\n",
    "    yt_pred = softmax(np.dot(Wy, a_next) +by)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # store values needed for backward propagation in cache\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
      "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
      "a_next.shape =  (5, 10)\n",
      "c_next[2] =  [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
      "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
      "c_next.shape =  (5, 10)\n",
      "yt[1] = [0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381\n",
      " 0.00943007 0.12666353 0.39380172 0.07828381]\n",
      "yt.shape =  (2, 10)\n",
      "cache[1][3] = [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
      "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
      "len(cache) =  10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "c_prev = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", c_next.shape)\n",
    "print(\"c_next[2] = \", c_next[2])\n",
    "print(\"c_next.shape = \", c_next.shape)\n",
    "print(\"yt[1] =\", yt[1])\n",
    "print(\"yt.shape = \", yt.shape)\n",
    "print(\"cache[1][3] =\", cache[1][3])\n",
    "print(\"len(cache) = \", len(cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**预期输出**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **a_next[4]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
    "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **a_next.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **c_next[2]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
    "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **c_next.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **yt[1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.79913913  0.15986619  0.22412122  0.15606108  0.97057211  0.31146381\n",
    "  0.00943007  0.12666353  0.39380172  0.07828381]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **yt.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (2, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cache[1][3]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
    "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **len(cache)**:\n",
    "        </td>\n",
    "        <td>\n",
    "           10\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - LSTM的正向传递\n",
    "\n",
    "现在，您已经实现了LSTM的一个步骤，现在可以使用for循环在$T_x$输入序列上对此进行迭代。\n",
    "\n",
    "<img src=\"images/LSTM_rnn.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **图4 **：多个时间步上的LSTM。</center></caption>\n",
    "\n",
    "**练习：**实现`lstm_forward()`以在$T_x$个时间步长上运行LSTM。\n",
    "\n",
    "**注意**：$c^{\\langle 0 \\rangle}$用零初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: lstm_forward\n",
    "\n",
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        Wi -- Weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bi -- Bias of the save gate, numpy array of shape (n_a, 1)\n",
    "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        Wo -- Weight matrix of the focus gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bo -- Bias of the focus gate, numpy array of shape (n_a, 1)\n",
    "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "                        \n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize \"caches\", which will track the list of all the caches\n",
    "    caches = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shapes of xt and Wy (≈2 lines)\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters['Wy'].shape\n",
    "    \n",
    "    # initialize \"a\", \"c\" and \"y\" with zeros (≈3 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next and c_next (≈2 lines)\n",
    "    a_next = a0\n",
    "    c_next = np.zeros((n_a, m))\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(x[:, :, t], a_next,c_next,parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y (≈1 line)\n",
    "        y[:,:,t] = yt\n",
    "        # Save the value of the next cell state (≈1 line)\n",
    "        c[:,:,t]  = c_next\n",
    "        # Append the cache into caches (≈1 line)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][3][6] =  0.17211776753291672\n",
      "a.shape =  (5, 10, 7)\n",
      "y[1][4][3] = 0.9508734618501101\n",
      "y.shape =  (2, 10, 7)\n",
      "caches[1][1[1]] = [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
      "  0.41005165]\n",
      "c[1][2][1] -0.8555449167181981\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "print(\"a[4][3][6] = \", a[4][3][6])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y[1][4][3] =\", y[1][4][3])\n",
    "print(\"y.shape = \", y.shape)\n",
    "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
    "print(\"c[1][2][1]\", c[1][2][1])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **a[4][3][6]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           0.172117767533\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **a.shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10, 7)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **y[1][4][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           0.95087346185\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **y.shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (2, 10, 7)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **caches[1][1][1]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
    "  0.41005165]\n",
    "        </td>    \n",
    "     </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **c[1][2][1]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           -0.855544916718\n",
    "        </td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **len(caches)** =\n",
    "        </td>\n",
    "        <td>\n",
    "           2\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜你！ 现在，您已经为基本RNN和LSTM实现了前向传递。 使用深度学习框架时，实施前向通过足以构建可实现出色性能的系统。\n",
    "\n",
    "此笔记本电脑的其余部分是可选的，不会评分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 递归神经网络中的反向传播（可选/取消）\n",
    "\n",
    "在现代深度学习框架中，您只需要实现前向传递，并且框架会处理后向传递，因此大多数深度学习工程师无需理会后向传递的细节。 但是，如果您是微积分专家并且想查看RNN中反向传播的详细信息，则可以遍历笔记本的此可选部分。\n",
    "\n",
    "在较早的课程中，当您实现了一个简单的（完全连接的）神经网络时，您就使用了反向传播来计算关于更新参数的成本的导数。 同样，在递归神经网络中，您可以计算成本的导数以更新参数。 反向传播方程非常复杂，我们在讲座中没有导出它们。 但是，我们将在下面简要介绍它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - 基本RNN向后传递\n",
    "\n",
    "我们将从计算基本RNN单元的反向传递开始。\n",
    "\n",
    "<img src=\"images/rnn_cell_backprop.png\" style=\"width:500;height:300px;\"> <br>\n",
    "<caption><center> **Figure 5**:RNN单元的后向传递。 就像在完全连接的神经网络中一样，成本函数$J$的导数通过遵循来自计算的链规则在RNN中反向传播。 链规则还用于计算$(\\frac{\\partial J}{\\partial W_{ax}},\\frac{\\partial J}{\\partial W_{aa}},\\frac{\\partial J}{\\partial b})$ 以更新参数以更新参数$(W_{ax}, W_{aa}, b_a)$。 </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推后一步功能：\n",
    "\n",
    "要计算 `rnn_cell_backward`，您需要计算以下方程式。 手工导出它们是一个很好的练习。\n",
    "\n",
    " tanh的导数为 $\\tanh$ is $1-\\tanh(x)^2$。 您可以在[here](https://www.wyzant.com/resources/lessons/math/calculus/derivative_proofs/tanx)。 请注意： $ \\sec(x)^2 = 1 - \\tanh(x)^2$\n",
    "\n",
    "同样，对于 $\\frac{ \\partial a^{\\langle t \\rangle} } {\\partial W_{ax}}, \\frac{ \\partial a^{\\langle t \\rangle} } {\\partial W_{aa}},  \\frac{ \\partial a^{\\langle t \\rangle} } {\\partial b}$的导数为$\\tanh(u)$ is $(1-\\tanh(u)^2)du$。\n",
    "\n",
    "最后两个方程式也遵循相同的规则，并使用$\\tanh$导数得出。 请注意，这种安排是为了获得相同的尺寸来匹配的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_cell_backward(da_next, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for the RNN-cell (single time-step).\n",
    "\n",
    "    Arguments:\n",
    "    da_next -- Gradient of loss with respect to next hidden state\n",
    "    cache -- python dictionary containing useful values (output of rnn_step_forward())\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradients of input data, of shape (n_x, m)\n",
    "                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dba -- Gradients of bias vector, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve values from cache\n",
    "    (a_next, a_prev, xt, parameters) = cache\n",
    "    \n",
    "    # Retrieve values from parameters\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # compute the gradient of tanh with respect to a_next (≈1 line)\n",
    "    dtanh = None\n",
    "\n",
    "    # compute the gradient of the loss with respect to Wax (≈2 lines)\n",
    "    dxt = None\n",
    "    dWax = None\n",
    "\n",
    "    # compute the gradient with respect to Waa (≈2 lines)\n",
    "    da_prev = None\n",
    "    dWaa = None\n",
    "\n",
    "    # compute the gradient with respect to b (≈1 line)\n",
    "    dba = None\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "b = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "\n",
    "da_next = np.random.randn(5,10)\n",
    "gradients = rnn_cell_backward(da_next, cache)\n",
    "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dxt\"][1][2]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           -0.460564103059\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dxt\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (3, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da_prev\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           0.0842968653807\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da_prev\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWax\"][3][1]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           0.393081873922\n",
    "        </td>\n",
    "    </tr>\n",
    "            <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWax\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 3)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWaa\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           -0.28483955787\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWaa\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 5)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dba\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.80517166]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dba\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向后通过RNN\n",
    "\n",
    "在每个时间步长$t$上计算相对于$a^{\\langle t \\rangle}$的成本梯度非常有用，因为它有助于将梯度反向传播到先前的RNN单元。 为此，您需要从头开始遍历所有时间步骤，并且在每一步中，增加整体$db_a$, $dW_{aa}$, $dW_{ax}$并存储$dx$ 。\n",
    "\n",
    "**说明**：\n",
    "\n",
    "实现`rnn_backward`函数。 首先用零初始化返回变量，然后循环遍历所有时间步长，同时在每个时间步长调用`rnn_cell_backward`，并相应地更新其他变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_backward(da, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for a RNN over an entire sequence of input data.\n",
    "\n",
    "    Arguments:\n",
    "    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)\n",
    "    caches -- tuple containing information from the forward pass (rnn_forward)\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)\n",
    "                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)\n",
    "                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)\n",
    "                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)\n",
    "                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "        \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve values from the first cache (t=1) of caches (≈2 lines)\n",
    "    (caches, x) = None\n",
    "    (a1, a0, x1, parameters) = None\n",
    "    \n",
    "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
    "    n_a, m, T_x = None\n",
    "    n_x, m = None\n",
    "    \n",
    "    # initialize the gradients with the right sizes (≈6 lines)\n",
    "    dx = None\n",
    "    dWax = None\n",
    "    dWaa = None\n",
    "    dba = None\n",
    "    da0 = None\n",
    "    da_prevt = None\n",
    "    \n",
    "    # Loop through all the time steps\n",
    "    for t in reversed(range(None)):\n",
    "        # Compute gradients at time step t. Choose wisely the \"da_next\" and the \"cache\" to use in the backward propagation step. (≈1 line)\n",
    "        gradients = None\n",
    "        # Retrieve derivatives from gradients (≈ 1 line)\n",
    "        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"]\n",
    "        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)\n",
    "        dx[:, :, t] = None\n",
    "        dWax += None\n",
    "        dWaa += None\n",
    "        dba += None\n",
    "        \n",
    "    # Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) \n",
    "    da0 = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "a, y, caches = rnn_forward(x, a0, parameters)\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = rnn_backward(da, caches)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dx\"][1][2]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           [-2.07101689 -0.59255627  0.02466855  0.01483317]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dx\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (3, 10, 4)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da0\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           -0.314942375127\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da0\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "         <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWax\"][3][1]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           11.2641044965\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWax\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 3)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWaa\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           2.30333312658\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWaa\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 5)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dba\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.74747722]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dba\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - LSTM 后向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 向后退一步\n",
    "\n",
    "LSTM向后传递比向前传递要复杂得多。我们在下面为您提供了LSTM向后传递的所有方程式。 （如果您喜欢微积分练习，请尝试自己从头开始进行演算。）\n",
    "\n",
    "### 3.2.2 门派生\n",
    "\n",
    "$$d \\Gamma_o^{\\langle t \\rangle} = da_{next}*\\tanh(c_{next}) * \\Gamma_o^{\\langle t \\rangle}*(1-\\Gamma_o^{\\langle t \\rangle})\\tag{7}$$\n",
    "\n",
    "$$d\\tilde c^{\\langle t \\rangle} = dc_{next}*\\Gamma_i^{\\langle t \\rangle}+ \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * i_t * da_{next} * \\tilde c^{\\langle t \\rangle} * (1-\\tanh(\\tilde c)^2) \\tag{8}$$\n",
    "\n",
    "$$d\\Gamma_u^{\\langle t \\rangle} = dc_{next}*\\tilde c^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * \\tilde c^{\\langle t \\rangle} * da_{next}*\\Gamma_u^{\\langle t \\rangle}*(1-\\Gamma_u^{\\langle t \\rangle})\\tag{9}$$\n",
    "\n",
    "$$d\\Gamma_f^{\\langle t \\rangle} = dc_{next}*\\tilde c_{prev} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * c_{prev} * da_{next}*\\Gamma_f^{\\langle t \\rangle}*(1-\\Gamma_f^{\\langle t \\rangle})\\tag{10}$$\n",
    "\n",
    "\n",
    "### 3.2.3参数导数\n",
    "\n",
    "$$ dW_f = d\\Gamma_f^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{11} $$\n",
    "$$ dW_u = d\\Gamma_u^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{12} $$\n",
    "$$ dW_c = d\\tilde c^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{13} $$\n",
    "$$ dW_o = d\\Gamma_o^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{14}$$\n",
    "\n",
    "\n",
    "要计算$db_f, db_u, db_c, db_o$您只需要在$d\\Gamma_f^{\\langle t \\rangle}, d\\Gamma_u^{\\langle t \\rangle}, d\\tilde c^{\\langle t \\rangle}, d\\Gamma_o^{\\langle t \\rangle}$的水平（轴=1）轴上求和，d tildec langlet rangle，d Gammao​​ langlet rangle。注意，您应该有`keep_dims = True`选项。\n",
    "\n",
    "最后，您将针对先前的隐藏状态，先前的内存状态和输入计算导数。\n",
    "\n",
    "$$ da_{prev} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c^{\\langle t \\rangle} + W_o^T * d\\Gamma_o^{\\langle t \\rangle} \\tag{15}$$\n",
    "\n",
    "在这里，等式13的权重是前n_a个（即$W_f = W_f[:n_a,:]$等...）\n",
    "\n",
    "$$ dc_{prev} = dc_{next}\\Gamma_f^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} * (1- \\tanh(c_{next})^2)*\\Gamma_f^{\\langle t \\rangle}*da_{next} \\tag{16}$$\n",
    "$$ dx^{\\langle t \\rangle} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c_t + W_o^T * d\\Gamma_o^{\\langle t \\rangle}\\tag{17} $$\n",
    "\n",
    "等式15的权重是从n_a到末尾（即$W_f = W_f[n_a:,:]$等...）\n",
    "\n",
    "**练习：**通过执行下面的等式$7-17$来实现`lstm_cell_backward`。祝好运！ :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for the LSTM-cell (single time-step).\n",
    "\n",
    "    Arguments:\n",
    "    da_next -- Gradients of next hidden state, of shape (n_a, m)\n",
    "    dc_next -- Gradients of next cell state, of shape (n_a, m)\n",
    "    cache -- cache storing information from the forward pass\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)\n",
    "                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)\n",
    "                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWi -- Gradient w.r.t. the weight matrix of the input gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
    "                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
    "                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
    "                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve information from \"cache\"\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from xt's and a_next's shape (≈2 lines)\n",
    "    n_x, m = None\n",
    "    n_a, m = None\n",
    "    \n",
    "    # Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines)\n",
    "    dot = None\n",
    "    dcct = None\n",
    "    dit = None\n",
    "    dft = None\n",
    "    \n",
    "    # Code equations (7) to (10) (≈4 lines)\n",
    "    dit = None\n",
    "    dft = None\n",
    "    dot = None\n",
    "    dcct = None\n",
    "\n",
    "    # Compute parameters related derivatives. Use equations (11)-(14) (≈8 lines)\n",
    "    dWf = None\n",
    "    dWi = None\n",
    "    dWc = None\n",
    "    dWo = None\n",
    "    dbf = None\n",
    "    dbi = None\n",
    "    dbc = None\n",
    "    dbo = None\n",
    "\n",
    "    # Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). (≈3 lines)\n",
    "    da_prev = None\n",
    "    dc_prev = None\n",
    "    dxt = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Save gradients in dictionary\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "c_prev = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
    "\n",
    "da_next = np.random.randn(5,10)\n",
    "dc_next = np.random.randn(5,10)\n",
    "gradients = lstm_cell_backward(da_next, dc_next, cache)\n",
    "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
    "print(\"gradients[\\\"dc_prev\\\"][2][3] =\", gradients[\"dc_prev\"][2][3])\n",
    "print(\"gradients[\\\"dc_prev\\\"].shape =\", gradients[\"dc_prev\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预期输出**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dxt\"][1][2]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           3.23055911511\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dxt\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (3, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da_prev\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           -0.0639621419711\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da_prev\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "         <tr>\n",
    "        <td>\n",
    "            **gradients[\"dc_prev\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           0.797522038797\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dc_prev\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWf\"][3][1]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           -0.147954838164\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWf\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWi\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           1.05749805523\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWi\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWc\"][3][1]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           2.30456216369\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWc\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWo\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           0.331311595289\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWo\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbf\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.18864637]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbf\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbi\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.40142491]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbi\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbc\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.25587763]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbc\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbo\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.13893342]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbo\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 向后传递LSTM RNN\n",
    "\n",
    "这部分与您在上面实现的`rnn_backward`函数非常相似。 首先，您将创建与返回变量相同维的变量。 然后，您将从头开始遍历所有时间步骤，并在每次迭代中调用为LSTM实现的一步功能。 然后，您将通过分别汇总参数来更新参数。 最后返回带有新渐变的字典。\n",
    "\n",
    "**指令**：实现`lstm_backward`函数。 创建一个从$T_x$开始并向后的for循环。 对于每个步骤，请调用`lstm_cell_backward`，并通过向其添加新渐变来更新旧渐变。 请注意，`dxt`不会更新而是存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).\n",
    "\n",
    "    Arguments:\n",
    "    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)\n",
    "    dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x)\n",
    "    caches -- cache storing information from the forward pass (lstm_forward)\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradient of inputs, of shape (n_x, m, T_x)\n",
    "                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
    "                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
    "                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
    "                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve values from the first cache (t=1) of caches.\n",
    "    (caches, x) = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
    "    n_a, m, T_x = None\n",
    "    n_x, m = None\n",
    "    \n",
    "    # initialize the gradients with the right sizes (≈12 lines)\n",
    "    dx = None\n",
    "    da0 = None\n",
    "    da_prevt = None\n",
    "    dc_prevt = None\n",
    "    dWf = None\n",
    "    dWi = None\n",
    "    dWc = None\n",
    "    dWo = None\n",
    "    dbf = None\n",
    "    dbi = None\n",
    "    dbc = None\n",
    "    dbo = None\n",
    "    \n",
    "    # loop back over the whole sequence\n",
    "    for t in reversed(range(None)):\n",
    "        # Compute all gradients using lstm_cell_backward\n",
    "        gradients = None\n",
    "        # Store or add the gradient to the parameters' previous step's gradient\n",
    "        dx[:,:,t] = None\n",
    "        dWf = None\n",
    "        dWi = None\n",
    "        dWc = None\n",
    "        dWo = None\n",
    "        dbf = None\n",
    "        dbi = None\n",
    "        dbc = None\n",
    "        dbo = None\n",
    "    # Set the first activation's gradient to the backpropagated gradient da_prev.\n",
    "    da0 = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = lstm_backward(da, caches)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dx\"][1][2]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.00173313  0.08287442 -0.30545663 -0.43281115]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dx\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (3, 10, 4)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da0\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           -0.095911501954\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da0\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWf\"][3][1]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           -0.0698198561274\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWf\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWi\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           0.102371820249\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWi\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWc\"][3][1]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           -0.0624983794927\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWc\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWo\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           0.0484389131444\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWo\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbf\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.0565788]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbf\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbi\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.06997391]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbi\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbc\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.27441821]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbc\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbo\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.16532821]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbo\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恭喜！\n",
    "\n",
    "祝贺您完成此作业。 您现在了解了递归神经网络的工作原理！\n",
    "\n",
    "让我们继续下一个练习，在该练习中，您将使用RNN来构建字符级语言模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "xxuVc",
   "launcher_item_id": "X20PE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
